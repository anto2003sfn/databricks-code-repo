{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "325a20c4-f2a3-4dc8-8dde-8cdb31d61a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Define the schema through simple & complex data types via multiple files\n",
    "\n",
    "Assumptions : <br>\n",
    "1. All files are same structure in terms of schema<br>\n",
    "   1.1 implementing simple schema<br>\n",
    "   1.2 implementing complex schema by enforcing through struct<br>\n",
    "2. Files are available in different location as source<br>\n",
    "3. Multiple files are avaialbe in same loacation with different directories<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d82c25d7-f2e5-4744-93e1-6cef6121d78e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "base_path = \"/Volumes/workspace/\"\n",
    "db_path = f\"{base_path}/db_schema_sales\"\n",
    "volume_path=f\"{db_path}/usage_metrics\"\n",
    "print(base_path);\n",
    "print(db_path);\n",
    "print(volume_path);\n",
    "# df = spark.read.format(\"delta\").load(volume_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d0ce66-bfe0-4ee6-8a1f-b89b260a7f96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType,DateType\n",
    ";\n",
    "\n",
    "schema = \"date DATE, mobile_ops_sys STRING, percentile DOUBLE\"\n",
    "df = spark.read.csv(path=[\"dbfs:///Volumes/workspace/db_schema_sales/usage_metrics/mobile_os_usage_1.csv\",\"dbfs:///Volumes/workspace/db_schema_sales/usage_metrics/mobile_os_usage_2.csv\"], header=True,schema=schema)\n",
    "# # display(df)/Volumes/workspace/db_schema_sales/usage_metrics/custs\n",
    "print(df.count())\n",
    "df.printSchema()\n",
    "# df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bd303c4-4cc9-4c85-8d2a-b5a3ea58b13e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8131df4c-f932-4fd4-a236-20c1c0f820c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType,DateType\n",
    ";\n",
    "\n",
    "schema = StructType([StructField(\"date\", DateType(), True),\n",
    "                     StructField(\"mobile_ops_sys\", StringType(), True),\n",
    "                     StructField(\"percentile\", DoubleType(), True)\n",
    "                    ])\n",
    "df = spark.read.csv(path=[\"dbfs:///Volumes/workspace/db_schema_sales/usage_metrics/mobile_os_usage_1.csv\",\"dbfs:///Volumes/workspace/db_schema_sales/usage_metrics/mobile_os_usage_2.csv\"], header=True,schema=schema)\n",
    "# # display(df)/Volumes/workspace/db_schema_sales/usage_metrics/custs\n",
    "print(df.count())\n",
    "df.printSchema()\n",
    "# df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2766163-5b3e-4e6f-84b4-8e2e9c1e2471",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark = getSparkSession()\n",
    "df = spark.read.csv(path=[f\"dbfs://{volume_path}/mobile_os_usage.csv\"\n",
    "                          ,f\"dbfs://{volume_path}/mobile_os_usage.csv\",\n",
    "                          f\"dbfs://{volume_path}/mobile_os_usage_1.csv\",\n",
    "                          f\"dbfs://{volume_path}/mobile_os_usage_2.csv\",],\n",
    "                    pathGlobFilter=\"*.csv\",\n",
    "                    header=True,sep=\",\",\n",
    "                    # inferSchema=True,\n",
    "                    schema=schema,\n",
    "                    recursiveFileLookup=True)\n",
    "\n",
    "print(df.count());\n",
    "# display(df)\n",
    "\n",
    "# df = spark.read.format(\"csv\").options(header=True, inferSchema=True, delimiter=\",\").load(f\"dbfs://{volume_path}/mobile_os_usage*.csv\").toDF(\"date\",\"mobile_ops_sys\",\"percentile\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "5_2_multiple_path_as_source",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
