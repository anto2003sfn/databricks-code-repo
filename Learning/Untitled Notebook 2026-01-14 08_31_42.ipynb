{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee13a56d-2294-4a4f-9e0b-829030e9ff3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "class TestStreamListener(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        # Create a local Spark Session for testing\n",
    "        cls.spark = SparkSession.builder.appName(\"TestingListener\").getOrCreate()\n",
    "\n",
    "    def test_listener_captures_progress(self):\n",
    "        # 1. Define a simple schema and a mock listener\n",
    "        schema = StructType([StructField(\"data\", StringType(), True)])\n",
    "        \n",
    "        class TestListener(StreamingQueryListener):\n",
    "            def __init__(self):\n",
    "                self.batch_count = 0\n",
    "                self.started = False\n",
    "            def onQueryStarted(self, event): self.started = True\n",
    "            def onQueryProgress(self, event): self.batch_count += 1\n",
    "            def onQueryTerminated(self, event): pass\n",
    "\n",
    "        test_listener = TestListener()\n",
    "        self.spark.streams.addListener(test_listener)\n",
    "\n",
    "        # 2. Use a Memory Stream to simulate incoming data\n",
    "        input_df = self.spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load()\n",
    "        \n",
    "        query = (input_df.writeStream\n",
    "                 .format(\"memory\")\n",
    "                 .queryName(\"test_query\")\n",
    "                 .start())\n",
    "\n",
    "        # 3. Wait a few seconds for micro-batches to process\n",
    "        time.sleep(5)\n",
    "        query.stop()\n",
    "\n",
    "        # 4. Assertions\n",
    "        self.assertTrue(test_listener.started, \"Listener should have detected query start\")\n",
    "        self.assertGreater(test_listener.batch_count, 0, \"Listener should have recorded at least one batch\")\n",
    "\n",
    "        # Cleanup\n",
    "        self.spark.streams.removeListener(test_listener)\n",
    "\n",
    "# Run the tests programmatically to avoid SystemExit\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestStreamListener)\n",
    "unittest.TextTestRunner(verbosity=2).run(suite)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2026-01-14 08_31_42",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
