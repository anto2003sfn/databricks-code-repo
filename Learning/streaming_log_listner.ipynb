{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "065b1217-088a-463a-a33f-7b205622b69b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "# 1. Define the Listener Class\n",
    "class MyStreamMonitor(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        print(f\"--- STREAM STARTED: {event.name if event.name else event.id} ---\")\n",
    "\n",
    "    def onQueryProgress(self, event):\n",
    "        # This runs every time a micro-batch finishes\n",
    "        progress = event.progress\n",
    "        batch_id = progress.batchId\n",
    "        rows_processed = progress.numInputRows\n",
    "        \n",
    "        print(f\"Batch: {batch_id} | Rows Processed: {rows_processed} | Input Rate: {progress.inputRowsPerSecond} rows/sec\")\n",
    "\n",
    "    def onQueryTerminated(self, event):\n",
    "        print(f\"--- STREAM STOPPED: {event.id} ---\")\n",
    "        if event.exception:\n",
    "            print(f\"Termination caused by Error: {event.exception}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70c9cad6-d36b-40f7-90f0-7481d87f2215",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Register the Listener to the Spark Session\n",
    "monitor = MyStreamMonitor()\n",
    "spark.streams.addListener(monitor)\n",
    "\n",
    "# Now, when you start any stream (like the 'query' above), \n",
    "# the 'onQueryProgress' method will fire automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9156b896-042c-49a5-afa7-0a3b61e7d54f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.streams.addListener(monitor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05756d0d-9ec4-41d9-a4f6-a5ed1f27c9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.streaming import StreamingQueryListener\n",
    "\n",
    "class DatabricksStreamMonitor(StreamingQueryListener):\n",
    "    def onQueryStarted(self, event):\n",
    "        print(f\"Stream '{event.name}' (ID: {event.id}) started.\")\n",
    "\n",
    "    def onQueryProgress(self, event):\n",
    "        progress = event.progress\n",
    "        # Log to the 'Log4j' system so it shows in the 'Driver Logs' tab\n",
    "        print(f\"Batch {progress.batchId} processed {progress.numInputRows} rows.\")\n",
    "        \n",
    "    def onQueryTerminated(self, event):\n",
    "        if event.exception:\n",
    "            print(f\"Stream terminated with error: {event.exception}\")\n",
    "        else:\n",
    "            print(\"Stream stopped successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3575f8bf-e3b1-41d8-82c1-7c3e1c013394",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 4"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Instantiate\n",
    "my_monitor = DatabricksStreamMonitor()\n",
    "\n",
    "# 2. Register the listener (no duplicate check possible in PySpark)\n",
    "spark.streams.addListener(my_monitor)\n",
    "print(\"âœ… Listener registered successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "streaming_log_listner",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
