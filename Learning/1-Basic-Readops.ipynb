{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c645f538-8fb6-4adb-b439-bfe7acbac568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Creating the catalog,schema and volume\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01f0c61f-5918-4bfc-904a-c28ac57b9dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists catalog1_dropme;\n",
    "create schema if not exists catalog1_dropme.schema_dropme;\n",
    "create volume if not exists  catalog1_dropme.schema_dropme.volume_dropme;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1763c97-ef96-4541-81b3-9f6db086963d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.mkdirs(\"/Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme\")\n",
    "#upload all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1215bad-0007-4540-98ae-21564a58ee87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# #Move all the data to a new directory\n",
    "# sourcePath = \"/Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/\"\n",
    "# files = dbutils.fs.ls (\"/Volumes/catalog1_dropme/schema_dropme/volume_dropme/\")\n",
    "\n",
    "# for file in files:\n",
    "#     # print(file.path)\n",
    "#     dbutils.fs.mv(file.path, sourcePath,True);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb3b2d9-5d3f-4cfa-ba92-9ec993dcb1f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls (\"/Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ec2e27b-4a3b-45cd-90cf-193935a563a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BasicReadOps\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc2d2f5-06b3-4f5b-b9e8-6799c3244f7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#If I don't use any options in this csv function, what is the default functionality?\n",
    "#1. By default it will consider ',' as a delimiter (sep='~')\n",
    "#2. By default it will use _c0,_c1..._cn it will apply as column headers (header=True or toDF(\"\",\"\",\"\") or we have more options to see further)\n",
    "#3. By default it will treat all columns as string (inferSchema=True or we have more options to see further)\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/custs\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f555464-0c50-4b3d-af47-ff58623af81c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Adding schema to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "347f68ba-4e7d-4914-b350-7d3e37ff2d96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "schemaStruct = StructType([\n",
    "    StructField(\"id\", StringType(),True),\n",
    "    StructField(\"fname\",StringType(),True),\n",
    "    StructField(\"lname\",StringType(),True),\n",
    "    StructField(\"age\",StringType(),True),\n",
    "    StructField(\"prof\",StringType(),True),\n",
    "])\n",
    "\n",
    "csv_df1=spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/custs\",schema=schemaStruct)\n",
    "csv_df1.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea27d7f2-962d-4d61-aa48-e84b400eca0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema=\"id string,fname string, lname string, age integer, prof string\"\n",
    "csv_df1 = spark.read.csv(\"dbfs:///Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/custs\",schema=schema)\n",
    "\n",
    "csv_df1.printSchema()\n",
    "display(csv_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ee28572-d794-4a83-bd61-69571a16b697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1.filter(\"prof is not null\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3487ab26-1692-4888-9dbc-b693e613e4f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "cust_schema=StructType([\n",
    "    StructField(\"id\", StringType(),True),\n",
    "    StructField(\"fname\", StringType(),True),\n",
    "    StructField(\"lname\", StringType(),True),\n",
    "    StructField(\"age\", StringType(),True),\n",
    "    StructField(\"prof\", StringType(),True),\n",
    "]);\n",
    "csv_df1 = spark.read.schema(cust_schema).csv(\"dbfs:///Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/custs\")\n",
    "\n",
    "csv_df1.printSchema()\n",
    "display(csv_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01761c11-48da-4528-a460-ee74aef3754b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df1.write.json(\"dbfs:///Volumes/catalog1_dropme/schema_dropme/volume_dropme/directory_dropme/custs_1\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7080257604371404,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Basic-Readops",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
